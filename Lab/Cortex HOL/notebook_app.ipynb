{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c84a2c0-ea96-4b15-b8d2-7eeb43f374c8",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "# phData - Snowflake Cortex HOL"
  },
  {
   "cell_type": "markdown",
   "id": "9af5ea6c-58a9-44ae-ace8-fb6881a25253",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## 1. Overview\n\nTo reduce hallucinations (i.e. incorrect responses), LLMs can be combined with private datasets. Today, the most common approach for reducing hallucinations without having to change the model (e.g. fine-tuning) is the Retrieval Augmented Generation (RAG) framework. RAG allows you to \"ground\" the model's responses by making a set of relevant documents available to the LLM as context in the response.\n\nIn this quickstart we will show you how to quickly and securely build a full-stack RAG application in Snowflake without having to build integrations, manage any infrastructure or deal with security concerns with data moving outside of the Snowflake governance framework.\n\nWe will show you how easy it is to implement RAG via a chat assistant that knows everything about smart devices. This assistant can be really useful for your not so tech-savvy friend or relative that is always asking you questions about their electronics. To make the assistant an expert in a smart devices, we are going to give it access to a few User Manuals. This template can easily be adapted to other documents that may be more interesting to you whether its financial reports, research documents or anything else!\n\nAlong the way, we will also share tips on how you could turn what may seem like a prototype into a production pipeline by showing you how to automatically process new documents as they are uploaded as well as learn about relevant Snowflake functionality to consider for additional enhancements.\n\n### What You Will Build\n\nThe final product includes an application that lets users test how the LLM responds with and without the context document(s) to show how RAG can address hallucinations.\n\n### What You Will Learn\n\nHow to create functions that use Python libraries using Snowpark\n\nHow to generate embeddings, run semantic search and use LLMs using serverless functions in Snowflake Cortex\n\nHow to build a front-end with Python using Streamlit in Snowflake\n\nOptional: How to automate data processing pipelines using directory tables, Streams and Task\n\n### Prerequisites\n\nSnowflake account in a cloud region where Snowflake Cortex LLM functions are supported\n\nCheck LLM availability to help you decide where you want to create your snowflake account\n\nA Snowflake account with Anaconda Packages enabled by ORGADMIN.\n\nSnowflake Cortex vector functions for semantic distance calculations along with VECTOR as a data type enabled."
  },
  {
   "cell_type": "markdown",
   "id": "5ed6821f-4797-46cb-a66d-cdfc9c555508",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## 2. Organize Documents and Create Pre-Processing Functions\n\nIn Snowflake, databases and schemas are used to organize and govern access to data and logic. LetÂ´s start by getting a few documents locally and then create a database that will hold the PDFs, the functions that will process (extract and chunk) those PDFs and the table that will hold the text embeddings.\n\n### Step 1. Download example documents\n\nLet's download a few documents we have created about bikes. In those documents we have added some very specific information about those ficticious models. You can always add more or use a different type of documents that you want to try asking questions against.\n\n### Step 2. Open a new Worksheet\n\nRelevant documentation: Creating Snowflake Worksheets.\n\n### Step 3. Create a database and a schema\n\nRun the following code inside your newly created worksheet:"
  },
  {
   "cell_type": "code",
   "id": "6a9bf230-61e5-4e8c-81af-95aaa75c2876",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- CREATE DATABASE CC_QUICKSTART_CORTEX_DOCS;\n-- CREATE SCHEMA DATA;\nUSE CC_QUICKSTART_CORTEX_DOCS.DATA;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a53c943c-a96a-4173-9161-68c1e5c6a5ec",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "### Step 4. Create a table function that will read the PDF documents and split them in chunks\n\nWe will be using the PyPDF2 and Langchain Python libraries to accomplish the necessary document processing tasks. Because as part of Snowpark Python these are available inside the integrated Anaconda repository, there are no manual installs or Python environment and dependency management required.\n\nCreate the function by running the following query inside your worksheet:"
  },
  {
   "cell_type": "code",
   "id": "95f08097-07b7-4893-aa7f-182fd8dff070",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "create or replace function pdf_text_chunker(file_url string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'pdf_text_chunker'\npackages = ('snowflake-snowpark-python','PyPDF2', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\nimport pandas as pd\n\nclass pdf_text_chunker:\n\n    def read_pdf(self, file_url: str) -> str:\n    \n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n    \n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n            \n        reader = PyPDF2.PdfReader(buffer)   \n        text = \"\"\n        for page in reader.pages:\n            try:\n                text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n            except:\n                text = \"Unable to Extract\"\n                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n        \n        return text\n\n    def process(self,file_url: str):\n\n        text = self.read_pdf(file_url)\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 4000, #Adjust this as you see fit\n            chunk_overlap  = 400, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dcdf399d-fd46-4b4a-bcea-5f94b28c94fb",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "### Step 5. Create a Stage with Directory Table where you will be uploading your documents:"
  },
  {
   "cell_type": "code",
   "id": "6117acdb-9dec-45f5-889c-d19c83c3206a",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51f880c4-6236-42d6-bc64-992a43d6bffb",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### Step 6. Upload documents to your staging area\n\n- Select Data on the left of Snowsight\n\n- Click on your database CC_QUICKSTART_CORTEX_DOCS\n\n- Click on your schema DATA\n\n- Click on Stages and select DOCS\n\n- On the top right click on the +Files botton\n\n- Drag and drop the two PDF files you downloaded\n\n### Step 7. Check files has been successfully uploaded\n\nRun this query to check what documents are in the staging area:\n"
  },
  {
   "cell_type": "code",
   "id": "64861c4d-5e20-49e2-bb3d-695c3f411c93",
   "metadata": {
    "language": "sql",
    "name": "cell10",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d0c4c54-d085-4d56-8501-2bc170292ccc",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": ""
  }
 ]
}